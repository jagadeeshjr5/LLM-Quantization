{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8427776,"sourceType":"datasetVersion","datasetId":5018456}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jagadeeshjr5/gemma-quantization?scriptVersionId=178323067\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport torch.nn.functional as F\n\nimport os\n\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-05-18T05:33:34.414162Z","iopub.execute_input":"2024-05-18T05:33:34.414764Z","iopub.status.idle":"2024-05-18T05:33:39.313288Z","shell.execute_reply.started":"2024-05-18T05:33:34.414729Z","shell.execute_reply":"2024-05-18T05:33:39.312503Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"execution":{"iopub.status.busy":"2024-05-18T05:33:39.314742Z","iopub.execute_input":"2024-05-18T05:33:39.315134Z","iopub.status.idle":"2024-05-18T05:33:39.340281Z","shell.execute_reply.started":"2024-05-18T05:33:39.315109Z","shell.execute_reply":"2024-05-18T05:33:39.339462Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca5e7b7ae8454ffbad74910a6519d223"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-2b-it\",\n    torch_dtype=torch.float32, device_map='cuda'\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T05:34:11.606599Z","iopub.execute_input":"2024-05-18T05:34:11.606993Z","iopub.status.idle":"2024-05-18T05:34:46.617528Z","shell.execute_reply.started":"2024-05-18T05:34:11.606965Z","shell.execute_reply":"2024-05-18T05:34:46.616819Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06bb60100780415280e8d6bfc418c8c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"815b8199da404ee7aa2cbbae68214ff0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2a8e752364b4b4f815c88b95f7e3f35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4973d62aad14661b9adfd9524a50490"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f50d6634026a4300858973f22eedc3e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d90b87fe76e1407cbaaf9e07fe5c584f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a687d6b3d55a4b2b97e1f42508011be5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88dadb84a18e4dfa921b70e4231b7c5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6efb31da43d49869d5d5e8792b9e87a"}},"metadata":{}},{"name":"stderr","text":"Gemma's activation function should be approximate GeLU and not exact GeLU.\nChanging the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54c1d5dfb3b6463e812988237dfbd759"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"894e20d09723415cbd87e7ed42cef61a"}},"metadata":{}}]},{"cell_type":"code","source":"torch.save(model.state_dict(), '/kaggle/working/gemma-2b-it.pth')","metadata":{"execution":{"iopub.status.busy":"2024-05-18T05:34:46.619152Z","iopub.execute_input":"2024-05-18T05:34:46.619552Z","iopub.status.idle":"2024-05-18T05:35:14.475453Z","shell.execute_reply.started":"2024-05-18T05:34:46.619527Z","shell.execute_reply":"2024-05-18T05:35:14.474427Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef scale(rmax, rmin, bits=8, mode='affine'):\n    if mode == 'affine':\n        qmax = 2**(bits - 1) - 1\n        qmin = -2**(bits - 1)\n        s = (rmax - rmin) / (qmax - qmin)\n    else:\n        s = rmax / 2**(bits - 1)\n    return s\n\ndef zero_point(s, rmin, bits=8, mode='affine'):\n    if mode == 'affine':\n        qmax = 2**(bits - 1) - 1\n        qmin = -2**(bits - 1)\n        z = np.round(qmin - (rmin / s))\n    else:\n        z = 0\n    return z\n\ndef quantize(x, bits=8, mode='affine', dtype=np.int8):\n    s = scale(x.max(), x.min(), bits)\n    z = zero_point(s, x.min(), bits)\n    qmax = 2**(bits - 1) - 1\n    qmin = -2**(bits - 1)\n    quantized_x = np.round(1 / s * x + z, decimals=0)\n    quantized_x = np.clip(quantized_x, a_min=qmin, a_max=qmax)\n    quantized_x = quantized_x.astype(dtype)\n    return quantized_x\n\ndef dequantize(qx, rmax, rmin, bits=8, mode='affine', dtype=np.float32):\n    s = scale(rmax, rmin, bits)\n    z = zero_point(s, rmin, bits)\n    qx = qx.astype(np.int32)\n    dequantized_x = s * (qx - z)\n    dequantized_x = dequantized_x.astype(dtype)\n    return dequantized_x","metadata":{"execution":{"iopub.status.busy":"2024-05-18T05:35:16.512125Z","iopub.execute_input":"2024-05-18T05:35:16.512768Z","iopub.status.idle":"2024-05-18T05:35:16.523609Z","shell.execute_reply.started":"2024-05-18T05:35:16.512736Z","shell.execute_reply":"2024-05-18T05:35:16.522748Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"quantized_model_parameters = {}\nminmax_values = {}\n\nfor key, value in model.state_dict().items():\n    value_np = value.cpu().numpy()\n    quantized_value_np = quantize(value_np)\n    quantized_value_tensor = torch.from_numpy(quantized_value_np)\n    quantized_model_parameters[key] = quantized_value_tensor\n    minmax_values[key] = [value.max().item(), value.min().item()]","metadata":{"execution":{"iopub.status.busy":"2024-05-18T05:35:16.670353Z","iopub.execute_input":"2024-05-18T05:35:16.670613Z","iopub.status.idle":"2024-05-18T05:35:51.43319Z","shell.execute_reply.started":"2024-05-18T05:35:16.670592Z","shell.execute_reply":"2024-05-18T05:35:51.432071Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"torch.save(quantized_model_parameters, \"gemma-2b-linearquantized.pth\")\ntorch.save(minmax_values, \"gemma-2b-minmax-parameters.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-05-18T05:35:51.435266Z","iopub.execute_input":"2024-05-18T05:35:51.435643Z","iopub.status.idle":"2024-05-18T05:35:54.679606Z","shell.execute_reply.started":"2024-05-18T05:35:51.435611Z","shell.execute_reply":"2024-05-18T05:35:54.6786Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print('gemma-2b-it size (GB):', np.round(os.path.getsize(\"/kaggle/working/gemma-2b-it.pth\")/1e9), 2)\nprint('Quantized gemma-2b-it size (GB):', np.round(os.path.getsize(\"/kaggle/working/gemma-2b-linearquantized.pth\")/1e9), 2)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T05:35:54.680857Z","iopub.execute_input":"2024-05-18T05:35:54.681205Z","iopub.status.idle":"2024-05-18T05:35:54.688277Z","shell.execute_reply.started":"2024-05-18T05:35:54.681166Z","shell.execute_reply":"2024-05-18T05:35:54.687322Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"gemma-2b-it size (GB): 10.0 2\nQuantized gemma-2b-it size (GB): 3.0 2\n","output_type":"stream"}]},{"cell_type":"code","source":"assert model.state_dict().keys() == torch.load('/kaggle/working/gemma-2b-linearquantized.pth', map_location=torch.device('cpu')).keys(), \"State dicts have different keys!\"\n\nmse_losses = []\n\nmodel.to('cpu')\n\nfor key in tqdm(model.state_dict().keys(), total=len(model.state_dict().keys())):\n    weight1 = model.state_dict()[key]\n    weight2 = dequantize(torch.load('/kaggle/working/gemma-2b-linearquantized.pth', map_location=torch.device('cpu'))[key].numpy(), torch.load('/kaggle/working/gemma-2b-minmax-parameters.pth', map_location=torch.device('cpu'))[key][0], torch.load('/kaggle/working/gemma-2b-minmax-parameters.pth', map_location=torch.device('cpu'))[key][1])\n    weight2 = torch.tensor(weight2, dtype=weight1.dtype)\n    mse_loss = F.mse_loss(weight1, weight2)\n    mse_losses.append(mse_loss.item())\n    \n\naverage_mse_loss = sum(mse_losses) / len(mse_losses)\n\n#print(\"Individual MSE losses:\", mse_losses)\nprint(\"Average MSE loss:\", average_mse_loss)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T05:35:54.690593Z","iopub.execute_input":"2024-05-18T05:35:54.690978Z","iopub.status.idle":"2024-05-18T05:41:33.927335Z","shell.execute_reply.started":"2024-05-18T05:35:54.690946Z","shell.execute_reply":"2024-05-18T05:41:33.926381Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 165/165 [05:29<00:00,  2.00s/it]","output_type":"stream"},{"name":"stdout","text":"Average MSE loss: 2.1192357436819715e-05\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}